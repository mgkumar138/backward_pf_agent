#%%
"""backward_shift_fields.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T-rStA1FSHl-MCL7iSJzaaWeHcd_7c1C
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib.colors import Normalize
import matplotlib.gridspec as gridspec
from scipy import stats
import argparse


import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--trials', type=int, required=False, help='trials', default=1000)
parser.add_argument('--gamma', type=float, required=False, help='gamma', default=0.9)
parser.add_argument('--eta0', type=float, required=False, help='eta0', default=0.1)
parser.add_argument('--N', type=int, required=False, help='N', default=1000)
parser.add_argument('--sigma_init', type=float, required=False, help='sigma', default=0.5)
parser.add_argument('--w_init', type=float, required=False, help='w_init', default=0.0)
parser.add_argument('--seed', type=int, required=False, help='seed', default=0)
parser.add_argument('--nstates', type=int, required=False, help='nstates', default=5)
args, unknown = parser.parse_known_args()
print(args)





# create environment
class Discrete1DEnv:
    def __init__(self, nstates=5):
        self.states = np.arange(nstates) # number of discrete states. can be increased or decreased. Similar to taking discrete steps to the reward
        self.start_state = 1 # always start from state 1, such that the agent starts from the screen, after choice initiation
        self.reward_state = self.states[-2] # reward/terminal state is 1 before the final state.
        self.done = False # if done, trial ends and a new one begins
        self.maxt = 100 # max number of steps available for the agent to get to the reward state, afterwhich the trial ends and a new one begins.

    def reset(self): # start trial
        self.current_state = self.start_state # current state is the agent's current state
        self.done = False
        self.time = 0 # time = 0 for start
        return self.current_state

    def step(self, action=None):
      self.time+=1 # take a step in time

      # end trial if agent takes maxt steps and no reward
      if self.time >= self.maxt:
        self.done = True

      if action is None: # if no action specified, use optimal policy to move right
        if self.current_state < self.reward_state:
            self.current_state += 1
        else:
            self.done = True # end trial if agent reaches reward state
      else:
        if action == 0:
          self.current_state -= 1 # move back
        elif action == 1:
          self.current_state += 1 # move forward
        elif action == 2:
          self.current_state += 0 # stay
        else:
          raise ValueError("Invalid action. Must be 0, 1, or None.")

      # boundary conditions to prevent circular/unbounded environment
      if self.current_state < 0:
        self.current_state = 0
      elif self.current_state > len(self.states)-1:
        self.current_state = len(self.states)-1

      # reward contingency: no reward in other states
      if self.current_state == self.reward_state:
        reward = 1
        self.done = True # end trial if agent reaches reward state
      else:
        reward = 0

      return self.current_state, reward, self.done

# Agent implementation
class Agent:
    def __init__(self, gamma=0.95, eta=0.5, N=50, nstates=5, learn_policy=False, pf_type='simple', policy_prop=False, beta_alpha=0.0, sigma_init=0.5, w_init=0.0, seed=0):
        np.random.seed(seed)
        self.N = N # number of place cells

        # check out different critic weight initialization influences place cell shifts.
        # self.w = np.ones(self.N) * 0 # critic's weight for value estimation, all constants.

        # self.w = np.random.uniform(-1,1,self.N) * 1 # random uniform critic weights

        self.w = np.random.normal(size=self.N) * w_init # random normal critic weights

        self.learn_policy = learn_policy # toggle True or False to learn a policy or use optimal policy
        self.pftype = pf_type # complexity of place cell with/without amplitude
        self.nact = 3 # move back,  forward, or stay
        self.W = np.zeros([self.nact, self.N]) # actor weight for policy learning
        self.lambdas = np.linspace(0, nstates-1, num=self.N) # initialized values for place field center of mass
        self.sigmas = np.ones(self.N) * sigma_init # initialized values for place field spread
        self.gamma0 = 1.0 # rich learning regime. keep to 1. originally 1/N for lazy regime
        # self.gamma0 = 1/self.N # weak learning regime
        self.gamma = gamma # TD error discount factor
        self.policy_prop = policy_prop

        # initialized values for place field amplitude
        if pf_type == 'alpha':
          self.alphas = np.ones(self.N) * 0
        else:
          self.alphas = np.ones(self.N) * 1.0
        self.beta_alpha = beta_alpha

        # self.gamma0 = 1/self.N # weak learning regime
        self.eta = eta * self.gamma0**2 # learning rate
        self.trial_data = {
            'values': [],
            'td_errors': [],
            'lambdas': [],
            'states':[],
            'latencies':[],
            'sigmas':[],
            'alphas':[]
        }

        if pf_type == 'simple':
          self.phi = self.phi_simple
        elif pf_type == 'alpha':
          self.phi = self.phi_alpha
        elif pf_type == 'norm':
          self.phi = self.phi_norm

    def phi_simple(self, s): # simple place cell model with constant amplitude = 1 and variable center
        return np.exp(-(s - self.lambdas)**2 / (2 * self.sigmas**2))

    def phi_alpha(self, s): # place cell with variable amplitdue and center
        return self.sigmoid(self.alphas) * np.exp(-(s - self.lambdas)**2 / (2 * self.sigmas**2))

    # def phi_alpha(self, s):
    #     return self.alphas**2 * np.exp(-(s - self.lambdas)**2 / (2 * self.sigmas**2))

    def phi_norm(self, s):
        # return (1 / (self.sigmoid(self.sigmas) * np.sqrt(2*np.pi))) * np.exp(-(s - self.lambdas)**2 / (2 * self.sigmoid(self.sigmas)**2))
        return (1 / (self.sigmas * np.sqrt(2*np.pi))) * np.exp(-(s - self.lambdas)**2 / (2 * self.sigmas**2))

    def value(self, s):# place cell with normalized amplitude
        return 1/(self.gamma0*self.N) * np.sum(self.w * self.phi(s))

    def sigmoid(self,x):
        return 1/(1+np.exp(-x))

    def get_action(self,s):
        a_t = 1/(self.gamma0*self.N) * np.sum(self.W * self.phi(s),axis=1)
        P_t = np.exp(a_t) / np.sum(np.exp(a_t))
        action = np.random.choice(self.nact, p=P_t)
        g_t = np.zeros(self.nact)
        g_t[action] = 1
        self.gtilde = g_t - P_t
        return action

    def run_trial(self, env):
        state = env.reset()
        done = False
        trial_values = []
        trial_td_errors = []
        states = []

        while not done:

            if self.learn_policy:
              action = self.get_action(state)
            else:
              action = None

            next_state, reward, done = env.step(action)

            # compute TD error
            v_s = self.value(state)
            v_next = self.value(next_state) if not done else 0
            td_error = reward + self.gamma * v_next - v_s

            # compute gradients
            phi_s = self.phi(state)

            # Update critic weights
            # dw = delta_t * phi_t
            delta_w = td_error * phi_s
            self.w += self.eta * delta_w /(self.gamma0*self.N)

            # add constraint on w to be positive semi-definite
            self.w = np.maximum(self.w, 0)

            if self.learn_policy:
              # update actor weights
              # dW = (g-P) @ phi_t * delta_t
              delta_W = self.gtilde[:,None] @ phi_s[None,:] * td_error
              self.W += self.eta * delta_W /(self.gamma0*self.N)

            # Update lambdas using only critic weights
            delta_lambda = td_error * self.w * phi_s * (state - self.lambdas) / self.sigmas
            if self.policy_prop:
              # Update lambdas using both actor & critic weights
              delta_lambda = td_error * (self.w + self.gtilde @ self.W) * phi_s * (state - self.lambdas) / self.sigmas
            self.lambdas += self.eta * delta_lambda /(self.gamma0*self.N)

            # update alphas
            if self.pftype == 'alpha':
              # delta_alpha = td_error * self.w * phi_s * (2/self.alphas)
              delta_alpha = td_error * self.w * phi_s * (1-self.sigmoid(self.alphas)) - self.beta_alpha * self.alphas
              # delta_alpha = td_error * self.w * phi_s * (2/self.alphas) - self.beta_alpha * self.alphas
              if self.policy_prop:
                  delta_alpha = td_error * (self.w + self.gtilde @ self.W) * phi_s * (1-self.sigmoid(self.alphas)) - self.beta_alpha * self.alphas
                  # delta_alpha = td_error * (self.w + self.gtilde @ self.W) * phi_s * (2/self.alphas) - self.beta_alpha * self.alphas
              self.alphas += self.eta * delta_alpha /(self.gamma0*self.N)

            # update sigmas
            # if self.pftype == 'sigma':
            #   delta_sigma = td_error * self.w * phi_s * (state - self.lambdas)**2 / self.sigmas**3
            #   self.sigmas += self.eta * delta_sigma

            # update norm
            if self.pftype == 'norm':
              delta_sigma = td_error * self.w * phi_s * (((state - self.lambdas)**2 / self.sigmas**3) - (1/self.sigmas))
              # delta_sigma = td_error * self.w * phi_s * ((((state - self.lambdas)**2 / self.sigmoid(self.sigmas)**2) -1) * self.sigmoid(self.sigmas)*(1-self.sigmoid(self.sigmas)))
              if self.policy_prop:
                delta_sigma = td_error * (self.w + self.gtilde @ self.W) * phi_s * (((state - self.lambdas)**2 / self.sigmas**3) - (1/self.sigmas))
                # delta_sigma = td_error * (self.w + self.gtilde @ self.W) * phi_s * ((((state - self.lambdas)**2 / self.sigmoid(self.sigmas)**2) -1) * self.sigmoid(self.sigmas)*(1-self.sigmoid(self.sigmas))  )
              self.sigmas += self.eta * delta_sigma /(self.gamma0*self.N)


            trial_values.append(v_s)
            trial_td_errors.append(td_error)
            states.append(state)
            state = next_state

        self.trial_data['values'].append(trial_values)
        self.trial_data['td_errors'].append(trial_td_errors)
        self.trial_data['lambdas'].append(self.lambdas.copy())
        self.trial_data['sigmas'].append(self.sigmas.copy())
        self.trial_data['alphas'].append(self.alphas.copy())
        self.trial_data['states'].append(states)
        self.trial_data['latencies'].append(env.time)

# Simulation
# agent params

learn_policy = False # If True, agent will learn a policy for noisier field updates. If False, agent uses optimal policy and we see smoother backward shifts
pf_type = 'simple' # 'simple', 'alpha', 'norm'
policy_prop = False # If True, update place cell parameters using policy weights too
beta_alpha = 0.0

gamma= args.gamma
sigma_init = args.sigma_init
w_init = args.w_init
N = args.N
eta0 = args.eta0
eta = eta0 * N
seed = args.seed

# assert that if policy_prop is True, learn_policy must be True
if policy_prop:
    assert learn_policy, "If policy_prop is True, learn_policy must be True"

# env params
num_trials = args.trials
nstates = args.nstates

env = Discrete1DEnv(nstates)
agent = Agent(gamma, eta,N, nstates,learn_policy, pf_type, policy_prop, beta_alpha, sigma_init, w_init, seed)

for t in range(num_trials):
    agent.run_trial(env)

    # add noise after N trials
    if (t+1)%20 == 0:
        agent.lambdas += np.random.normal(size=N) * 0.0

# Prepare data for plotting
states = env.states
trials = range(1, num_trials + 1)

# Create 3D arrays for values and TD errors (trials Ã— states)
value_matrix = np.zeros((num_trials, len(states)))
td_matrix = np.zeros((num_trials, len(states)))

for trial in range(num_trials):
    for i, state in enumerate(agent.trial_data['states'][trial]):
        value_matrix[trial, state] = agent.trial_data['values'][trial][i]
        td_matrix[trial, state] = agent.trial_data['td_errors'][trial][i]

# Lambda values for each trial
sigmas = np.array(agent.trial_data['sigmas'])
lambdas = np.array(agent.trial_data['lambdas'])
alphas = np.array(agent.trial_data['alphas'])

# plt.figure(figsize=(9,2))
# plt.subplot(131)
# plt.plot(agent.trial_data['latencies'])
# plt.ylabel('Latency')
# plt.xlabel('Trial')
# plt.subplot(132)
# plt.plot(agent.w)
# plt.ylabel('Weight')
# plt.xlabel('N')
# plt.subplot(133)
# plt.imshow(agent.W,aspect='auto')
# plt.colorbar()
# plt.tight_layout()


#%%
# Percentage of cells across time.

perc_reward_cells = []
perc_reward_approach_cells = []
perc_screen_cells = []

for i in range(num_trials):
  rp_idx1 = lambdas[i] < env.reward_state+0.25
  rp_idx2 = lambdas[i] > env.reward_state-.75
  rp_idx = rp_idx1 * rp_idx2
  perc_reward_cells.append(int(np.sum(rp_idx)))

  rp_idx1 = lambdas[i] < env.reward_state-3.00
  rp_idx2 = lambdas[i] > env.reward_state-4.00
  rp_idx = rp_idx1 * rp_idx2
  perc_reward_approach_cells.append(int(np.sum(rp_idx)))

  sc_idx1 = lambdas[i] < env.start_state+0.5
  sc_idx2 = lambdas[i] > env.start_state-0.25
  sc_idx  = sc_idx1 * sc_idx2
  perc_screen_cells.append(int(np.sum(sc_idx)))


perc_reward_cells = 100*np.array(perc_reward_cells)/num_trials
perc_reward_approach_cells = 100*np.array(perc_reward_approach_cells)/num_trials
perc_screen_cells = 100*np.array(perc_screen_cells)/num_trials

indices = np.arange(0, num_trials, 10)
# plt.figure()
# plt.plot(indices, perc_reward_cells[indices], marker = 'o',  label='reward cells')
# plt.plot(indices, perc_reward_approach_cells[indices], marker = 'o', label='reward approach cells')
# plt.plot(indices, perc_screen_cells[indices], marker = 'o', label='screen cells')
# plt.xlabel('trial number')
# plt.ylabel('percentage of cells')
# plt.legend(fontsize=10)
# plt.show()


# %%

# store change in perc_reward_cells etc
df_perc_reward_cells = perc_reward_cells[-1]- perc_reward_cells[0]
df_perc_reward_approach_cells = perc_reward_approach_cells[-1]- perc_reward_approach_cells[0]
df_perc_screen_cells = perc_screen_cells[-1]- perc_screen_cells[0]


print(f'Percentage of reward cells: {df_perc_reward_cells}')
print(f'Percentage of reward approach cells: {df_perc_reward_approach_cells}')
print(f'Percentage of screen cells: {df_perc_screen_cells}')
np.savez(f'./data/dcells_{sigma_init}sigma_{w_init}w_{eta0}eta0_{N}N_{gamma}gamma_{nstates}states_{seed}seed_td.npz',   
         df_perc_reward_cells=df_perc_reward_cells, df_perc_reward_approach_cells=df_perc_reward_approach_cells, df_perc_screen_cells=df_perc_screen_cells, 
         tdf=np.mean(0.5*td_matrix[-1]**2))